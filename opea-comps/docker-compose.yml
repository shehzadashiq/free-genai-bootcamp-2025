# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

version: '3.8'
services:
  ollama-server:
    image: ollama/ollama
    container_name: ollama-server
    env_file:
      - .env
    ports:
      - "11434:11434"
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      host_ip: ${host_ip}
      LLM_MODEL_ID: ${LLM_MODEL_ID}

  tgi-server:
    # image: ghcr.io/huggingface/text-generation-inference:2.4.0-intel-cpu
    image: ghcr.io/huggingface/text-generation-inference:2.4.0
    container_name: tgi-server
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - ${TGI_PORT:-8080}:80
    volumes:
      - "${DATA_PATH:-./data}:/data"
      - "./huggingface:/root/.cache/huggingface"
    shm_size: 1g
    deploy:
      resources:
        limits:
          cpus: '8'  # Increase CPU allocation
          memory: 16G  # Increase memory allocation
        reservations:
          cpus: '4'
          memory: 8G
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
      HUGGING_FACE_TOKEN: ${HUGGING_FACE_TOKEN}
      HF_TOKEN: ${HUGGING_FACE_TOKEN}
      MAX_INPUT_TOKENS: ${MAX_INPUT_TOKENS:-2048}
      MAX_TOTAL_TOKENS: ${MAX_TOTAL_TOKENS:-4096}
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/health || exit 1"]
      interval: 30s
      timeout: 60s
      retries: 10
      start_period: 600s
    # command: --model-id ${SAFETY_GUARD_MODEL_ID} --cuda-graphs 0
    # command: --model-id "meta-llama/LlamaGuard-7b" --cuda-graphs 0
    # command: --model-id "meta-llama/LlamaGuard-7b-hf" --max-input-tokens 2048 --max-total-tokens 4096 --max-batch-prefill-tokens 2048 --quantize bitsandbytes --trust-remote-code    
    command: --model-id "meta-llama/Meta-Llama-Guard-2-8B" --cuda-graphs 0 --quantize bitsandbytes
    # command: --model-id "hf-internal-testing/tiny-random-LlamaForCausalLM" --cuda-graphs 0

  guardrails-service:
    image: opea/guardrails:latest
    container_name: guardrails-service
    env_file:
      - .env
    ports:
      - ${GUARDRAILS_PORT:-9090}:9090
    environment:
      no_proxy: ${no_proxy}
      http_proxy: ${http_proxy}
      https_proxy: ${https_proxy}
      HUGGING_FACE_HUB_TOKEN: ${HUGGING_FACE_HUB_TOKEN}
      HUGGING_FACE_TOKEN: ${HUGGING_FACE_TOKEN}
      HF_TOKEN: ${HUGGING_FACE_TOKEN}
      SAFETY_GUARD_ENDPOINT: "http://tgi-server:80"
      GUARDRAILS_COMPONENT_NAME: ${GUARDRAILS_COMPONENT_NAME}
      host_ip: ${host_ip}
      LLM_ENDPOINT_PORT: ${TGI_PORT:-8080}
    depends_on:
      tgi-server:
        condition: service_healthy

  mega-service:
    build:
      context: .
      dockerfile: mega-service/Dockerfile
    container_name: mega-service
    env_file:
      - .env
    ports:
      - "8000:8000"
    environment:
      LLM_SERVICE_HOST_IP: ollama-server
      LLM_SERVICE_PORT: 11434
      GUARDRAILS_SERVICE_HOST_IP: guardrails-service
      GUARDRAILS_PORT: 9090
      EMBEDDING_SERVICE_HOST_IP: ${EMBEDDING_SERVICE_HOST_IP:-embedding-service}
      EMBEDDING_SERVICE_PORT: ${EMBEDDING_SERVICE_PORT:-6000}
      SAFETY_GUARD_ENDPOINT: "http://tgi-server:80"
      host_ip: ${host_ip}
      LLM_ENDPOINT_PORT: ${TGI_PORT:-8080}
    depends_on:
      tgi-server:
        condition: service_started
      guardrails-service:
        condition: service_started

networks:
  default:
    name: opea-network
